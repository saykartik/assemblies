{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.datasets\n",
    "sys.path.append('../')\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Repository imports.\n",
    "from FFBrainNet import FFBrainNet\n",
    "from LocalNetBase import Options, UpdateScheme\n",
    "from DataGenerator import random_halfspace_data\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology in this notebook: For every model, meta-learn on 4-dimensional halfspace, then train / test on 8-dimensional halfspace by transferring rules across networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_simple(brain_up, brain_down, n_up, n_down):\n",
    "    # Upstream.\n",
    "    N = 1000\n",
    "    X,y = random_halfspace_data(dim=n_up, n=3*N)\n",
    "    X_test = X[:N]\n",
    "    y_test = y[:N]\n",
    "    X_train = X[N:]\n",
    "    y_train = y[N:]\n",
    "\n",
    "    print('Meta-learning...')\n",
    "    data_up = metalearn_rules(\n",
    "        X_train, y_train, brain_up, num_rule_epochs=20, num_epochs=2, batch_size=100, learn_rate=1e-2,\n",
    "        X_test=X_test, y_test=y_test, verbose=False)\n",
    "    \n",
    "    # Transfer rules.\n",
    "    try:\n",
    "        brain_down.set_hidden_layer_rule(brain_up.get_hidden_layer_rule())\n",
    "        brain_down.set_output_rule(brain_up.get_output_rule())\n",
    "    except:\n",
    "        brain_down.set_rnn_rule(brain_up.get_rnn_rule())\n",
    "        brain_down.set_output_rule(brain_up.get_output_rule())\n",
    "    \n",
    "    # Downstream.\n",
    "    N = 1000\n",
    "    X, y = random_halfspace_data(dim=n_down, n=3*N)\n",
    "    X_test = X[:N]\n",
    "    y_test = y[:N]\n",
    "    X_train = X[N:]\n",
    "    y_train = y[N:]\n",
    "    \n",
    "    print('Training...')\n",
    "    data_down = train_downstream(\n",
    "        X_train, y_train, brain_down, num_epochs=5, batch_size=100, vanilla=False, learn_rate=1e-2,\n",
    "        X_test=X_test, y_test=y_test, verbose=False, stats_interval=500)\n",
    "    \n",
    "    return (data_up, data_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(data_up, data_down):\n",
    "    (meta_losses, meta_train_acc, meta_test_acc, meta_sample_counts, meta_stats) = data_up\n",
    "    (plas_losses, plas_train_acc, plas_test_acc, plas_sample_counts, plas_stats) = data_down\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax[0].plot(meta_sample_counts, meta_losses, label='loss')\n",
    "    ax[0].plot(meta_sample_counts, meta_train_acc, label='train')\n",
    "    ax[0].plot(meta_sample_counts, meta_test_acc, label='test')\n",
    "    ax[0].set_xlabel('Cumulative number of training samples')\n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[0].set_title('Upstream meta-learning on 4-dim half-space')\n",
    "    ax[0].legend()\n",
    "    \n",
    "    ax[1].plot(plas_sample_counts[1:], plas_losses[1:], label='loss')\n",
    "    ax[1].plot(plas_sample_counts, plas_train_acc, label='train')\n",
    "    ax[1].plot(plas_sample_counts, plas_test_acc, label='test')\n",
    "    ax[1].set_xlabel('Cumulative number of training samples')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_title('Downstream training on 8-dim half-space')\n",
    "    ax[1].legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_models(datas_up, datas_down, labels):\n",
    "    num_models = len(datas_up)\n",
    "    assert(num_models == len(datas_down) and num_models == len(labels))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    for i in range(num_models):\n",
    "        ax[0].plot(datas_up[i][3], datas_up[i][2], label=labels[i])\n",
    "        ax[1].plot(datas_down[i][3], datas_down[i][2], label=labels[i])\n",
    "        \n",
    "    ax[0].set_xlabel('Cumulative number of training samples')\n",
    "    ax[0].set_ylabel('Test accuracy')\n",
    "    ax[0].set_title('Upstream meta-learning on 4-dim half-space')\n",
    "    ax[0].legend()\n",
    "    ax[1].set_xlabel('Cumulative number of training samples')\n",
    "    ax[1].set_ylabel('Test accuracy')\n",
    "    ax[1].set_title('Downstream training on 8-dim half-space')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, test original RNN as sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Henceforth, we use GD directly on inputs but use plasticity rules in the output and hidden layers.\n",
    "opts_up = Options(gd_input=True,\n",
    "                  use_graph_rule=True,\n",
    "                  gd_graph_rule=True,\n",
    "                  use_output_rule=True,\n",
    "                  gd_output_rule=True,\n",
    "                  gd_output=False)\n",
    "opts_down = Options(gd_input=True,\n",
    "                    use_graph_rule=True,\n",
    "                    gd_graph_rule=False,  # Not meta-trainable anymore!\n",
    "                    use_output_rule=True,\n",
    "                    gd_output_rule=False,  # Not meta-trainable anymore!\n",
    "                    gd_output=False)\n",
    "scheme = UpdateScheme(cross_entropy_loss=True,\n",
    "                      mse_loss=False,\n",
    "                      update_misclassified_only=False,\n",
    "                      update_all_edges=True)\n",
    "n_up = 4  # Input layer size for meta-learning.\n",
    "n_down = 8  # Input layer size for desired task training.\n",
    "m = 2  # Output layer size.\n",
    "l = 2  # Number of hidden layers.\n",
    "w = 10  # Width of hidden layers.\n",
    "p = 0.5  # Connectivity probability.\n",
    "cap = 5  # Number of nodes firing per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import *\n",
    "brain_rnn_up = LocalNet(n_up, m, 100, p, 50, 3, options=opts_up, update_scheme=scheme)\n",
    "brain_rnn_down = LocalNet(n_down, m, 100, p, 50, 3, options=opts_down, update_scheme=scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                  | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Original RNN (very different from all the rest) ====\n",
      "Meta-learning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:12<01:10,  4.13s/it]"
     ]
    }
   ],
   "source": [
    "print('==== Original RNN (very different from all the rest) ====')\n",
    "data_rnn = evaluate_simple(brain_rnn_up, brain_rnn_down, n_up, n_down)\n",
    "plot_curves(*data_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, test all table-based feed-forward networks by Brett."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FFLocalTableRules.FFLocalTable_PrePost import FFLocalTable_PrePost\n",
    "from FFLocalTableRules.FFLocalTable_PrePostCount import FFLocalTable_PrePostCount\n",
    "from FFLocalTableRules.FFLocalTable_PrePostPercent import FFLocalTable_PrePostPercent\n",
    "from FFLocalTableRules.FFLocalTable_PostCount import FFLocalTable_PostCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models.\n",
    "brain_prepost_up = FFLocalTable_PrePost(n_up, m, l, w, p, cap, options=opts_up, update_scheme=scheme)\n",
    "brain_prepost_down = FFLocalTable_PrePost(n_down, m, l, w, p, cap, options=opts_down, update_scheme=scheme)\n",
    "brain_prepostcount_up = FFLocalTable_PrePostCount(n_up, m, l, w, p, cap, options=opts_up, update_scheme=scheme)\n",
    "brain_prepostcount_down = FFLocalTable_PrePostCount(n_down, m, l, w, p, cap, options=opts_down, update_scheme=scheme)\n",
    "brain_prepostpercent_up = FFLocalTable_PrePostPercent(n_up, m, l, w, p, cap, options=opts_up, update_scheme=scheme)\n",
    "brain_prepostpercent_down = FFLocalTable_PrePostPercent(n_down, m, l, w, p, cap, options=opts_down, update_scheme=scheme)\n",
    "brain_postcount_up = FFLocalTable_PostCount(n_up, m, l, w, p, cap, options=opts_up, update_scheme=scheme)\n",
    "brain_postcount_down = FFLocalTable_PostCount(n_down, m, l, w, p, cap, options=opts_down, update_scheme=scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate models.\n",
    "print('==== Interpretation: Pre and Post ====')\n",
    "data_prepost = evaluate_simple(brain_prepost_up, brain_prepost_down, n_up, n_down)\n",
    "plot_curves(*data_prepost)\n",
    "print('==== Interpretation: Pre and Post and Incoming Count ====')\n",
    "data_prepostcount = evaluate_simple(brain_prepostcount_up, brain_prepostcount_down, n_up, n_down)\n",
    "plot_curves(*data_prepostcount)\n",
    "print('==== Interpretation: Pre and Post and Binned Incoming Fraction ====')\n",
    "data_prepostpercent = evaluate_simple(brain_prepostpercent_up, brain_prepostpercent_down, n_up, n_down)\n",
    "plot_curves(*data_prepostpercent)\n",
    "print('==== Interpretation: Post and Incoming Count ====')\n",
    "data_postcount = evaluate_simple(brain_postcount_up, brain_postcount_down, n_up, n_down)\n",
    "plot_curves(*data_postcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [data_rnn, data_prepost, data_prepostcount, data_prepostpercent, data_postcount]\n",
    "labels = ['RNN', 'PrePost', 'PrePostCount', 'PrePostPercent', 'PostCount']\n",
    "plot_compare_models([x[0] for x in datas], [x[1] for x in datas], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
