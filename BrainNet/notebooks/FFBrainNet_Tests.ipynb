{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.datasets\n",
    "sys.path.append('../')\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Repository imports.\n",
    "from FFBrainNet import FFBrainNet\n",
    "from LocalNetBase import Options, UpdateScheme\n",
    "from DataGenerator import random_halfspace_data\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide which form of plasticity rule to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FFLocalTableRules.FFLocalTable_PrePostCount import FFLocalTable_PrePostCount\n",
    "PlasRuleClass = FFLocalTable_PrePostCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train an FFBrainNet using regular gradient descent on all parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some test data\n",
    "N = 1000\n",
    "X,y = random_halfspace_data(dim=4, n=3*N)\n",
    "X_test = X[:N]\n",
    "y_test = y[:N]\n",
    "X_train = X[N:]\n",
    "y_train = y[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a multi-layer, randomly connected, and capped feed-forward net.\n",
    "brain = FFBrainNet(n=4, m=2, l=4, w=20, p=0.5, cap=5, full_gd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the net using regular backprop on the weights.\n",
    "(all_losses, all_train_acc, all_test_acc, sample_counts, other_stats) = train_downstream(\n",
    "    X_train, y_train, brain, num_epochs=10, batch_size=100, vanilla=True,\n",
    "    learn_rate=0.1, X_test=X_test, y_test=y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print essential stats.\n",
    "print(\"Last loss: {0:.4f}\".format(all_losses[-1]))\n",
    "print(\"Last train accuracy: {0:.4f}\".format(all_train_acc[-1]))\n",
    "print(\"Last test accuracy: {0:.4f}\".format(all_test_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot downstream training curves.\n",
    "plt.figure()\n",
    "plt.plot(sample_counts[1:], all_losses[1:], label='loss')\n",
    "plt.plot(sample_counts, all_train_acc, label='train')\n",
    "plt.plot(sample_counts, all_test_acc, label='test')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Regular backprop learning curves')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Meta-Learn an Output-layer Plasticity Rule on halfspace data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "dimension = 4\n",
    "N = 1000\n",
    "X, y = random_halfspace_data(dim = dimension, n = 3*N)\n",
    "X_test = X[:N]\n",
    "y_test = y[:N]\n",
    "X = X[N:]\n",
    "y = y[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options to meta-learn output layer plasticity rule, while using regular GD on input weights directly\n",
    "opts = Options(gd_input=True,\n",
    "               use_output_rule=True,\n",
    "               gd_output_rule=True)\n",
    "scheme = UpdateScheme(update_misclassified_only=False, update_all_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate an FFLocalNet with 1 hidden layer, width 10\n",
    "brain = PlasRuleClass(n=4, m=2, l=1, w=10, p=0.5, cap=5, options=opts, update_scheme=scheme)\n",
    "\n",
    "# Print initial output rule\n",
    "print('brain output_rule:')\n",
    "print(brain.get_output_rule())  # zero initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Meta-Learn a plasticity rule for the output layer\n",
    "(meta_losses, meta_train_acc, meta_test_acc, meta_sample_counts, meta_stats) = metalearn_rules(\n",
    "    X, y, brain, num_rule_epochs=20, num_epochs=2, batch_size=100, learn_rate=1e-2,\n",
    "    X_test=X_test, y_test=y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show updated output layer rule\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "print('brain output_layer_rule:')\n",
    "print(brain.get_output_rule())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print essential stats.\n",
    "print(\"Last loss: {0:.4f}\".format(meta_losses[-1]))\n",
    "print(\"Last train accuracy: {0:.4f}\".format(meta_train_acc[-1]))\n",
    "print(\"Last test accuracy: {0:.4f}\".format(meta_test_acc[-1]))\n",
    "\n",
    "# Plot upstream training curves.\n",
    "plt.figure()\n",
    "plt.plot(meta_sample_counts, meta_losses, label='loss')\n",
    "plt.plot(meta_sample_counts, meta_train_acc, label='train')\n",
    "plt.plot(meta_sample_counts, meta_test_acc, label='test')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plasticity-based upstream meta-learning curves')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the **same data** using the learned plasticity rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: The output weights will be automatically reset during the first sample within train_given_rule()\n",
    "(plas_losses, plas_train_acc, plas_test_acc, plas_sample_counts, plas_stats) = train_downstream(\n",
    "    X, y, brain, num_epochs=1, batch_size=1, vanilla=False, learn_rate=1e-2,\n",
    "    X_test=X_test, y_test=y_test, verbose=False, stats_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print essential stats.\n",
    "print(\"Last loss: {0:.4f}\".format(meta_losses[-1]))\n",
    "print(\"Last train accuracy: {0:.4f}\".format(meta_train_acc[-1]))\n",
    "print(\"Last test accuracy: {0:.4f}\".format(meta_test_acc[-1]))\n",
    "\n",
    "# Plot downstream training curves.\n",
    "plt.figure()\n",
    "plt.plot(plas_sample_counts[1:], plas_losses[1:], label='loss')\n",
    "plt.plot(plas_sample_counts, plas_train_acc, label='train')\n",
    "plt.plot(plas_sample_counts, plas_test_acc, label='test')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plasticity-based downstream learning curves')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with the already learned rule on a **different** halfspace:\n",
    "\n",
    "**NOTE**: The input weights learned from previous data will still be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data.\n",
    "dimension = 4\n",
    "N = 3000\n",
    "X, y = random_halfspace_data(dim=dimension, n=3*N)\n",
    "X_test = X[:N]\n",
    "y_test = y[:N]\n",
    "X = X[N:]\n",
    "y = y[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: The output weights will be automatically reset during the first sample within train_given_rule()\n",
    "(plas_losses, plas_train_acc, plas_test_acc, plas_sample_counts, plas_stats) = train_downstream(\n",
    "    X, y, brain, num_epochs=1, batch_size=1, vanilla=False, learn_rate=1e-2,\n",
    "    X_test=X_test, y_test=y_test, verbose=False, stats_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print essential stats.\n",
    "print(\"Last loss: {0:.4f}\".format(meta_losses[-1]))\n",
    "print(\"Last train accuracy: {0:.4f}\".format(meta_train_acc[-1]))\n",
    "print(\"Last test accuracy: {0:.4f}\".format(meta_test_acc[-1]))\n",
    "\n",
    "# Plot downstream training curves.\n",
    "plt.figure()\n",
    "plt.plot(plas_sample_counts[1:], plas_losses[1:], label='loss')\n",
    "plt.plot(plas_sample_counts, plas_train_acc, label='train')\n",
    "plt.plot(plas_sample_counts, plas_test_acc, label='test')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plasticity-based downstream learning curves')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does re-learning **input weights** for this new data improve the performance of the output plasticity rule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new network with the same params as before\n",
    "brain2 = PlasRuleClass(n=4, m=2, l=1, w=10, p=0.5, cap=5, options=opts, update_scheme=scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Learn input weights (via GD) for the new data\n",
    "# NOTE: We'll also be meta-learning a new output layer rule, but we'll throw it away later\n",
    "(meta_losses, meta_train_acc, meta_test_acc, meta_sample_counts, meta_stats) = metalearn_rules(\n",
    "    X, y, brain2, num_rule_epochs=20, num_epochs=2, batch_size=100, learn_rate=1e-2,\n",
    "    X_test=X_test, y_test=y_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the output layer plasticity rule with the rule we learned from the original data\n",
    "brain2.output_rule = nn.Parameter(brain.output_rule.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try learning again with the original output layer plasticity rule, but with input weights GD-learned for this data\n",
    "# NOTE: The output weights will be automatically reset during the first sample within train_given_rule()\n",
    "(plas_losses, plas_train_acc, plas_test_acc, plas_sample_counts, plas_stats) = train_downstream(\n",
    "    X, y, brain2, num_epochs=1, batch_size=1, vanilla=False, learn_rate=1e-2,\n",
    "    X_test=X_test, y_test=y_test, verbose=False, stats_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print essential stats.\n",
    "print(\"Last loss: {0:.4f}\".format(meta_losses[-1]))\n",
    "print(\"Last train accuracy: {0:.4f}\".format(meta_train_acc[-1]))\n",
    "print(\"Last test accuracy: {0:.4f}\".format(meta_test_acc[-1]))\n",
    "\n",
    "# Plot downstream training curves.\n",
    "plt.figure()\n",
    "plt.plot(plas_sample_counts[1:], plas_losses[1:], label='loss')\n",
    "plt.plot(plas_sample_counts, plas_train_acc, label='train')\n",
    "plt.plot(plas_sample_counts, plas_test_acc, label='test')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plasticity-based downstream learning curves')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this performance is significantly better than the previous plot, then the input weights are important. \\\n",
    "So, **YES** - the input weights make a big difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Meta-Learn a Hidden-layer Plasticity Rule on halfspace data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "dimension = 4\n",
    "N = 1000\n",
    "X, y = random_halfspace_data(dim = dimension, n = 3*N)\n",
    "X_test = X[:N]\n",
    "y_test = y[:N]\n",
    "X = X[N:]\n",
    "y = y[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set options to meta-learn a hidden layer plasticity rule, while using regular GD on input and output weights directly\n",
    "opts = Options(use_graph_rule=True,\n",
    "               gd_graph_rule=True,\n",
    "               gd_input=True,\n",
    "               gd_output=True)\n",
    "scheme = UpdateScheme(update_misclassified_only=False, update_all_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate an FFLocalNet with 2 hidden layers, width 20\n",
    "brain = PlasRuleClass(n=4, m=2, l=2, w=20, p=0.5, cap=10, options=opts, update_scheme=scheme)\n",
    "\n",
    "# Print initial rule\n",
    "print('brain hidden_layer_rule:')\n",
    "print(brain.get_hidden_layer_rule())  # randomly initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Meta-Learn a single plasticity rule between the two hidden layers\n",
    "(meta_losses, meta_train_acc, meta_test_acc, meta_sample_counts, meta_stats) = metalearn_rules(\n",
    "    X, y, brain, num_rule_epochs=20, num_epochs=2, batch_size=100, learn_rate=1e-2,\n",
    "    X_test=X_test, y_test=y_test, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_true_y, all_pred_y = meta_stats[:2]\n",
    "# all_true_y = np.array(all_true_y, dtype=np.int32)\n",
    "# all_pred_y = np.array(all_pred_y, dtype=np.int32)\n",
    "# plt.hist(all_true_y, bins=2)\n",
    "# plt.hist(all_pred_y, bins=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show updated hidden layer rule\n",
    "print('brain hidden_layer_rule:')\n",
    "print(brain.get_hidden_layer_rule())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print essential stats.\n",
    "print(\"Last loss: {0:.4f}\".format(meta_losses[-1]))\n",
    "print(\"Last train accuracy: {0:.4f}\".format(meta_train_acc[-1]))\n",
    "print(\"Last test accuracy: {0:.4f}\".format(meta_test_acc[-1]))\n",
    "\n",
    "# Plot upstream training curves.\n",
    "plt.figure()\n",
    "plt.plot(meta_sample_counts, meta_losses, label='loss')\n",
    "plt.plot(meta_sample_counts, meta_train_acc, label='train')\n",
    "plt.plot(meta_sample_counts, meta_test_acc, label='test')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plasticity-based upstream meta-learning curves')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the **same data** using the learned plasticity rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: The hidden layer weights will be automatically reset during the first sample within train_given_rule()\n",
    "(plas_losses, plas_train_acc, plas_test_acc, plas_sample_counts, plas_stats) = train_downstream(\n",
    "    X, y, brain, num_epochs=1, batch_size=1, vanilla=False, learn_rate=1e-2,\n",
    "    X_test=X_test, y_test=y_test, verbose=False, stats_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print essential stats.\n",
    "print(\"Last loss: {0:.4f}\".format(meta_losses[-1]))\n",
    "print(\"Last train accuracy: {0:.4f}\".format(meta_train_acc[-1]))\n",
    "print(\"Last test accuracy: {0:.4f}\".format(meta_test_acc[-1]))\n",
    "\n",
    "# Plot downstream training curves.\n",
    "plt.figure()\n",
    "plt.plot(plas_sample_counts[1:], plas_losses[1:], label='loss')\n",
    "plt.plot(plas_sample_counts, plas_train_acc, label='train')\n",
    "plt.plot(plas_sample_counts, plas_test_acc, label='test')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plasticity-based downstream learning curves')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with the already learned rule on a **different** halfspace:\n",
    "\n",
    "**NOTE**: the input and output weights learned from previous data will still be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data.\n",
    "dimension = 4\n",
    "N = 5000\n",
    "X, y = random_halfspace_data(dim=dimension, n=3*N)\n",
    "X_test = X[:N]\n",
    "y_test = y[:N]\n",
    "X = X[N:]\n",
    "y = y[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: The hidden layer weights will be automatically reset during the first sample within train_given_rule()\n",
    "(plas_losses, plas_train_acc, plas_test_acc, plas_sample_counts, plas_stats) = train_downstream(\n",
    "    X, y, brain, num_epochs=1, batch_size=1, vanilla=False, learn_rate=1e-2,\n",
    "    X_test=X_test, y_test=y_test, verbose=False, stats_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print essential stats.\n",
    "print(\"Last loss: {0:.4f}\".format(meta_losses[-1]))\n",
    "print(\"Last train accuracy: {0:.4f}\".format(meta_train_acc[-1]))\n",
    "print(\"Last test accuracy: {0:.4f}\".format(meta_test_acc[-1]))\n",
    "\n",
    "# Plot downstream training curves.\n",
    "plt.figure()\n",
    "plt.plot(plas_sample_counts[1:], plas_losses[1:], label='loss')\n",
    "plt.plot(plas_sample_counts, plas_train_acc, label='train')\n",
    "plt.plot(plas_sample_counts, plas_test_acc, label='test')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plasticity-based downstream learning curves')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does re-learning **input & output weights** for this new data improve the performance of the hidden layer plasticity rule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new network with the same params as before\n",
    "brain2 = PlasRuleClass(n=4, m=2, l=2, w=20, p=0.5, cap=10, options=opts, update_scheme=scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Learn input & output weights (via GD) for the new data\n",
    "# NOTE: We'll also be meta-learning a new hidden layer rule, but we'll throw it away later\n",
    "(meta_losses, meta_train_acc, meta_test_acc, meta_sample_counts, meta_stats) = metalearn_rules(\n",
    "    X, y, brain2, num_rule_epochs=20, num_epochs=2, batch_size=100, learn_rate=1e-2,\n",
    "    X_test=X_test, y_test=y_test, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the hidden layer plasticity rule with the rule we learned from the original data\n",
    "brain2.hidden_layer_rule = nn.Parameter(brain.hidden_layer_rule.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try learning again with the original hidden layer plasticity rule, but with input & output weights GD-learned for this data\n",
    "# NOTE: The hidden layer weights will be automatically reset during the first sample within train_given_rule()\n",
    "(plas_losses, plas_train_acc, plas_test_acc, plas_sample_counts, plas_stats) = train_downstream(\n",
    "    X, y, brain2, num_epochs=1, batch_size=1, vanilla=False, learn_rate=1e-2,\n",
    "    X_test=X_test, y_test=y_test, verbose=False, stats_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print essential stats.\n",
    "print(\"Last loss: {0:.4f}\".format(meta_losses[-1]))\n",
    "print(\"Last train accuracy: {0:.4f}\".format(meta_train_acc[-1]))\n",
    "print(\"Last test accuracy: {0:.4f}\".format(meta_test_acc[-1]))\n",
    "\n",
    "# Plot downstream training curves.\n",
    "plt.figure()\n",
    "plt.plot(plas_sample_counts[1:], plas_losses[1:], label='loss')\n",
    "plt.plot(plas_sample_counts, plas_train_acc, label='train')\n",
    "plt.plot(plas_sample_counts, plas_test_acc, label='test')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plasticity-based downstream learning curves')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this performance is significantly better than the previous plot, then the input & output weights are important. \\\n",
    "So, **YES** - the input & output weights make a big difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Output Layer meta-learning by recreating **Figure 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in MNIST\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "X_train = np.array([np.array(pair[0]).flatten() for pair in mnist_train]) / 255.0\n",
    "y_train = np.array([pair[1] for pair in mnist_train])\n",
    "X_test = np.array([np.array(pair[0]).flatten() for pair in mnist_test]) / 255.0\n",
    "y_test = np.array([pair[1] for pair in mnist_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with full GD on MNIST with one hidden layer, w=1000 \\\n",
    "This should be similar to the benchmark GD case for Figure 2 (GD-Trained w/ batch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brain = FFBrainNet(n=784, m=10, l=1, w=1000, p=0.1, cap=100, full_gd=True)\n",
    "(gd_losses, gd_train_acc, gd_test_acc, gd_sample_counts, gd_stats) = train_downstream(\n",
    "    X_train, y_train, brain, num_epochs=2, batch_size=200, vanilla=True, learn_rate=1e-3,\n",
    "    X_test=X_test, y_test=y_test, verbose=False, stats_interval=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print essential stats.\n",
    "print(\"Last loss: {0:.4f}\".format(gd_losses[-1]))\n",
    "print(\"Last train accuracy: {0:.4f}\".format(gd_train_acc[-1]))\n",
    "print(\"Last test accuracy: {0:.4f}\".format(gd_test_acc[-1]))\n",
    "\n",
    "# Plot downstream training curves.\n",
    "plt.figure()\n",
    "plt.plot(gd_sample_counts[1:], gd_losses[1:], label='loss')\n",
    "plt.plot(gd_sample_counts, gd_train_acc, label='train')\n",
    "plt.plot(gd_sample_counts, gd_test_acc, label='test')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Backprop learning curves on MNIST (batch size = 200)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare this to learning via an output plasticity rule. \\\n",
    "We'll first meta-learn an output rule using MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Learn an output layer plasticity rule\n",
    "opts = Options(use_output_rule=True, gd_output_rule=True, gd_input=True)\n",
    "scheme = UpdateScheme(update_misclassified_only=False, update_all_edges=True)\n",
    "local_brain = PlasRuleClass(n=784, m=10, l=1, w=1000, p=0.1, cap=100, options=opts, update_scheme=scheme)\n",
    "\n",
    "(meta_losses, meta_train_acc, meta_test_acc, meta_sample_counts, meta_stats) = metalearn_rules(\n",
    "    X_train, y_train, local_brain, num_rule_epochs=10, num_epochs=1, batch_size=100, learn_rate=1e-3,\n",
    "    X_test=X_test, y_test=y_test, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print essential stats.\n",
    "print(\"Last loss: {0:.4f}\".format(meta_losses[-1]))\n",
    "print(\"Last train accuracy: {0:.4f}\".format(meta_train_acc[-1]))\n",
    "print(\"Last test accuracy: {0:.4f}\".format(meta_test_acc[-1]))\n",
    "\n",
    "# Plot upstream training curves.\n",
    "plt.figure()\n",
    "plt.plot(meta_sample_counts, meta_losses, label='loss')\n",
    "plt.plot(meta_sample_counts, meta_train_acc, label='train')\n",
    "plt.plot(meta_sample_counts, meta_test_acc, label='test')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plasticity-based upstream meta-learning curves on MNIST')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try plasticity-based training using the learned output rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: The output weights will be automatically reset during the first sample within train_given_rule()\n",
    "(plas_losses, plas_train_acc, plas_test_acc, plas_sample_counts, plas_stats) = train_downstream(\n",
    "    X_train, y_train, local_brain, num_epochs=2, batch_size=200, vanilla=False, learn_rate=1e-3,\n",
    "    X_test=X_test, y_test=y_test, verbose=False, stats_interval=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print essential stats.\n",
    "print(\"Last loss: {0:.4f}\".format(meta_losses[-1]))\n",
    "print(\"Last train accuracy: {0:.4f}\".format(meta_train_acc[-1]))\n",
    "print(\"Last test accuracy: {0:.4f}\".format(meta_test_acc[-1]))\n",
    "\n",
    "# Plot downstream training curves.\n",
    "plt.figure()\n",
    "plt.plot(plas_sample_counts[1:], plas_losses[1:], label='loss')\n",
    "plt.plot(plas_sample_counts, plas_train_acc, label='train')\n",
    "plt.plot(plas_sample_counts, plas_test_acc, label='test')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plasticity-based downstream learning curves on MNIST')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the learning curves.\n",
    "plt.plot(plas_sample_counts, plas_train_acc, label='Rule-Trained')\n",
    "plt.plot(gd_sample_counts, gd_train_acc, label='GD-Trained (batch 200)')\n",
    "plt.xlabel('Cumulative number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Convergence of Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
