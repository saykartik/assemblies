{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing plasticity rules\n",
    "\n",
    "## For every model, meta-learn on 8-dimensional halfspace dataset, then transfer rules to a NEW instance, and train / test that on the same dataset type but with fixed rules.\n",
    "\n",
    "Created by Basile Van Hoorick, Fall 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run FF_common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Henceforth, we use GD directly on inputs but use plasticity rules in the output and hidden layers.\n",
    "opts_up = Options(gd_input=True,\n",
    "                  use_graph_rule=True,\n",
    "                  gd_graph_rule=True,\n",
    "                  use_output_rule=True,\n",
    "                  gd_output_rule=True,\n",
    "                  gd_output=False)\n",
    "opts_down = Options(gd_input=True,\n",
    "                    use_graph_rule=True,\n",
    "                    gd_graph_rule=False,  # Not meta-trainable anymore!\n",
    "                    use_output_rule=True,\n",
    "                    gd_output_rule=False,  # Not meta-trainable anymore!\n",
    "                    gd_output=False)\n",
    "if 0:\n",
    "    scheme = UpdateScheme(cross_entropy_loss=True,\n",
    "                          mse_loss=False,\n",
    "                          update_misclassified_only=False,\n",
    "                          update_all_edges=True)\n",
    "else:\n",
    "    # Same as paper.\n",
    "    scheme = UpdateScheme(cross_entropy_loss=True,\n",
    "                          mse_loss=False,\n",
    "                          update_misclassified_only=True,\n",
    "                          update_all_edges=False)\n",
    "\n",
    "# Feed-forward brain config.\n",
    "n_up = 8  # Input layer size for meta-learning.\n",
    "n_down = 8  # Input layer size for desired task training.\n",
    "m = 2  # Output layer size.\n",
    "l = 3  # Number of hidden layers.\n",
    "w = 50  # Width of hidden layers.\n",
    "p = 0.5  # Connectivity probability.\n",
    "cap = 25  # Number of nodes firing per layer.\n",
    "\n",
    "# Training config.\n",
    "num_runs = 10\n",
    "num_rule_epochs = 50\n",
    "num_epochs_upstream = 1\n",
    "num_epochs_downstream = 1\n",
    "downstream_backprop = False\n",
    "dataset_up = 'halfspace'\n",
    "dataset_down = 'halfspace'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate brain factories.\n",
    "brain_rnn_up_fact = lambda: LocalNet(n_up, m, w, p, cap, 2, options=opts_up, update_scheme=scheme)\n",
    "brain_rnn_down_fact = lambda: LocalNet(n_down, m, w, p, cap, 2, options=opts_down, update_scheme=scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Original RNN (very different from all the rest) ====\n",
      "\n",
      "Run 1 / 10...\n",
      "Meta-learning on halfspace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [00:45<00:54,  1.93s/it]"
     ]
    }
   ],
   "source": [
    "# Evaluate model.\n",
    "print('==== Original RNN (very different from all the rest) ====')\n",
    "stats_rnn_up, stats_rnn_down = evaluate_up_down(\n",
    "    brain_rnn_up_fact, brain_rnn_down_fact, n_up, n_down,\n",
    "    dataset_up=dataset_up, dataset_down=dataset_down,\n",
    "    downstream_backprop=downstream_backprop, num_runs=num_runs, num_rule_epochs=num_rule_epochs,\n",
    "    num_epochs_upstream=num_epochs_upstream, num_epochs_downstream=num_epochs_downstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot aggregated stats.\n",
    "agg_stats_rnn_up = convert_multi_stats_uncertainty(stats_rnn_up)\n",
    "agg_stats_rnn_down = convert_multi_stats_uncertainty(stats_rnn_down)\n",
    "plot_curves(agg_stats_rnn_up, agg_stats_rnn_down,\n",
    "            '[RNN] Upstream meta-learning on ' + dataset,\n",
    "            '[RNN] Downstream training on ' + dataset,\n",
    "            'figs/comparing_transfer_rnn_' + dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate all table-based feed-forward networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate brain factories.\n",
    "brain_prepost_up_fact = lambda: FFLocalNet(\n",
    "    n_up, m, l, w, p, cap, hl_rules=TableRule_PrePost(),\n",
    "    output_rule=TableRule_PrePost(), options=opts_up, update_scheme=scheme)\n",
    "brain_prepost_down_fact = lambda: FFLocalNet(\n",
    "    n_down, m, l, w, p, cap, hl_rules=TableRule_PrePost(),\n",
    "    output_rule=TableRule_PrePost(), options=opts_down, update_scheme=scheme)\n",
    "brain_prepostcount_up_fact = lambda: FFLocalNet(\n",
    "    n_up, m, l, w, p, cap, hl_rules=TableRule_PrePostCount(),\n",
    "    output_rule=TableRule_PrePostCount(), options=opts_up, update_scheme=scheme)\n",
    "brain_prepostcount_down_fact = lambda: FFLocalNet(\n",
    "    n_down, m, l, w, p, cap, hl_rules=TableRule_PrePostCount(),\n",
    "    output_rule=TableRule_PrePostCount(), options=opts_down, update_scheme=scheme)\n",
    "brain_prepostpercent_up_fact = lambda: FFLocalNet(\n",
    "    n_up, m, l, w, p, cap, hl_rules=TableRule_PrePostPercent(),\n",
    "    output_rule=TableRule_PrePostPercent(), options=opts_up, update_scheme=scheme)\n",
    "brain_prepostpercent_down_fact = lambda: FFLocalNet(\n",
    "    n_down, m, l, w, p, cap, hl_rules=TableRule_PrePostPercent(),\n",
    "    output_rule=TableRule_PrePostPercent(), options=opts_down, update_scheme=scheme)\n",
    "brain_postcount_up_fact = lambda: FFLocalNet(\n",
    "    n_up, m, l, w, p, cap, hl_rules=TableRule_PostCount(),\n",
    "    output_rule=TableRule_PostCount(), options=opts_up, update_scheme=scheme)\n",
    "brain_postcount_down_fact = lambda: FFLocalNet(\n",
    "    n_down, m, l, w, p, cap, hl_rules=TableRule_PostCount(),\n",
    "    output_rule=TableRule_PostCount(), options=opts_down, update_scheme=scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate models.\n",
    "print('==== Interpretation: Pre and Post ====')\n",
    "stats_prepost_up, stats_prepost_down = evaluate_up_down(\n",
    "    brain_prepost_up_fact, brain_prepost_down_fact, n_up, n_down,\n",
    "    dataset_up=dataset_up, dataset_down=dataset_down,\n",
    "    downstream_backprop=downstream_backprop, num_runs=num_runs, num_rule_epochs=num_rule_epochs,\n",
    "    num_epochs_upstream=num_epochs_upstream, num_epochs_downstream=num_epochs_downstream)\n",
    "print('==== Interpretation: Pre and Post and Incoming Count ====')\n",
    "stats_prepostcount_up, stats_prepostcount_down = evaluate_up_down(\n",
    "    brain_prepostcount_up_fact, brain_prepostcount_down_fact, n_up, n_down,\n",
    "    dataset_up=dataset_up, dataset_down=dataset_down,\n",
    "    downstream_backprop=downstream_backprop, num_runs=num_runs, num_rule_epochs=num_rule_epochs,\n",
    "    num_epochs_upstream=num_epochs_upstream, num_epochs_downstream=num_epochs_downstream)\n",
    "print('==== Interpretation: Pre and Post and Binned Incoming Fraction ====')\n",
    "stats_prepostpercent_up, stats_prepostpercent_down = evaluate_up_down(\n",
    "    brain_prepostpercent_up_fact, brain_prepostpercent_down_fact, n_up, n_down,\n",
    "    dataset_up=dataset_up, dataset_down=dataset_down,\n",
    "    downstream_backprop=downstream_backprop, num_runs=num_runs, num_rule_epochs=num_rule_epochs,\n",
    "    num_epochs_upstream=num_epochs_upstream, num_epochs_downstream=num_epochs_downstream)\n",
    "print('==== Interpretation: Post and Incoming Count ====')\n",
    "stats_postcount_up, stats_postcount_down = evaluate_up_down(\n",
    "    brain_postcount_up_fact, brain_postcount_down_fact, n_up, n_down,\n",
    "    dataset_up=dataset_up, dataset_down=dataset_down,\n",
    "    downstream_backprop=downstream_backprop, num_runs=num_runs, num_rule_epochs=num_rule_epochs,\n",
    "    num_epochs_upstream=num_epochs_upstream, num_epochs_downstream=num_epochs_downstream,\n",
    "    min_upstream_acc=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot aggregated stats.\n",
    "agg_stats_prepost_up = convert_multi_stats_uncertainty(stats_prepost_up)\n",
    "agg_stats_prepost_down = convert_multi_stats_uncertainty(stats_prepost_down)\n",
    "plot_curves(agg_stats_prepost_up, agg_stats_prepost_down,\n",
    "            '[PrePost] Upstream meta-learning on ' + dataset,\n",
    "            '[PrePost] Downstream training on ' + dataset,\n",
    "            'figs/comparing_transfer_prepost_' + dataset)\n",
    "agg_stats_prepostcount_up = convert_multi_stats_uncertainty(stats_prepostcount_up)\n",
    "agg_stats_prepostcount_down = convert_multi_stats_uncertainty(stats_prepostcount_down)\n",
    "plot_curves(agg_stats_prepostcount_up, agg_stats_prepostcount_down,\n",
    "            '[PrePostCount] Upstream meta-learning on ' + dataset,\n",
    "            '[PrePostCount] Downstream training on ' + dataset,\n",
    "            'figs/comparing_transfer_prepostcount_' + dataset)\n",
    "agg_stats_prepostpercent_up = convert_multi_stats_uncertainty(stats_prepostpercent_up)\n",
    "agg_stats_prepostpercent_down = convert_multi_stats_uncertainty(stats_prepostpercent_down)\n",
    "plot_curves(agg_stats_prepostpercent_up, agg_stats_prepostpercent_down,\n",
    "            '[PrePostPercent] Upstream meta-learning on ' + dataset,\n",
    "            '[PrePostPercent] Downstream training on ' + dataset,\n",
    "            'figs/comparing_transfer_prepostpercent_' + dataset)\n",
    "agg_stats_postcount_up = convert_multi_stats_uncertainty(stats_postcount_up)\n",
    "agg_stats_postcount_down = convert_multi_stats_uncertainty(stats_postcount_down)\n",
    "plot_curves(agg_stats_postcount_up, agg_stats_postcount_down,\n",
    "            '[PostCount] Upstream meta-learning on ' + dataset,\n",
    "            '[PostCount] Downstream training on ' + dataset,\n",
    "            'figs/comparing_transfer_postcount_' + dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to compare all.\n",
    "all_stats_up = [agg_stats_rnn_up, agg_stats_prepost_up, agg_stats_prepostcount_up, agg_stats_prepostpercent_up, agg_stats_postcount_up]\n",
    "all_stats_down = [agg_stats_rnn_down, agg_stats_prepost_down, agg_stats_prepostcount_down, agg_stats_prepostpercent_down, agg_stats_postcount_down]\n",
    "labels = ['RNN', 'PrePost', 'PrePostCount', 'PrePostPercent', 'PostCount']\n",
    "plot_compare_models(all_stats_up, all_stats_down, labels,\n",
    "                    'Upstream meta-learning on ' + dataset,\n",
    "                    'Downstream training on ' + dataset,\n",
    "                    'figs/comparing_transfer_table_' + dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    agg_stats_prepost_up = convert_multi_stats_uncertainty(stats_prepost_up)\n",
    "    agg_stats_prepost_down = convert_multi_stats_uncertainty(stats_prepost_down)\n",
    "    for i in range(len(stats_prepost_up)):\n",
    "        plot_curves(stats_prepost_up[i], stats_prepost_down[i],\n",
    "                    '[PrePost] Upstream meta-learning on ' + dataset,\n",
    "                    '[PrePost] Downstream training on ' + dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
